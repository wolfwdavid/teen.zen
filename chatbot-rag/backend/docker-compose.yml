version: '3'
services:
  inference-engine:
    container_name: inference
    image: ghcr.io/huggingface/text-generation-inference:1.4.0
    network_mode: "host"
    shm_size: '1g'
    restart: unless-stopped
    command: --model-id NousResearch/Nous-Capybara-34B
    environment:
      MAX_INPUT_LENGTH: 4096
      MAX_TOTAL_TOKENS: 8192
      CUDA_MEMORY_FRACTION: 0.9
      HUGGINGFACE_HUB_CACHE: /data
      PORT: 8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - /data:/data
