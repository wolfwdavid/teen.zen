services:
  tgi-cpu:
    image: ghcr.io/huggingface/text-generation-inference:1.4.0
    container_name: tgi-cpu
    environment:
      - MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - HF_HUB_ENABLE_HF_TRANSFER=1
    command:
      - --model-id
      - ${MODEL_ID}
      - --max-input-length
      - "1024"
      - --max-total-tokens
      - "1536"
      - --json-output
    ports:
      - "8080:80"       # host:container
    volumes:
      - ./data:/data    # cache models here
    restart: unless-stopped
